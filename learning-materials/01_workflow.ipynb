{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ffffff; color: #000000; padding: 10px;\">\n",
    "<div style=\"display: flex; justify-content: space-between; align-items: center; background-color: #ffffff; color: #000000; padding: 10px;\">\n",
    "    <img src=\"../images/logo_kisz.png\" height=\"80\" style=\"margin-right: auto;\" alt=\"Logo of the AI Service Center Berlin-Brandenburg.\">\n",
    "    <img src=\"../images/logo_bmbf.jpeg\" height=\"150\" style=\"margin-left: auto;\" alt=\"Logo of the German Federal Ministry of Education and Research: Gefördert vom Bundesministerium für Bildung und Forschung.\">\n",
    "</div>\n",
    "<h1> Efficient Information Retrieval from Documents\n",
    "<h2> Local Retrieval Augmented Generation System\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f6a800; color: #ffffff; padding: 10px;\">\n",
    "    <h2> Part 1 - Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3>1. Basics - Handcrafted RAG</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db5956ba11174ed6805f82a68dcd6d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fb96be7a55e4de8b9a3a0ff86a5b5e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "735bfa73c74b4df1830cc5b1176cd58d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c857ed458d643ef9656f338e792f155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b20a1d99cc6944418e6e541b55e74d68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa0bba97a6749479389a03be98f9a62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e6791b4d4848b9b9b24fc3736b2568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be67c4ff83746108ae4a0ac5c17b262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652b67bcce0d4ad597982475705d8e55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd8c2890b0b45d9aee3e7250da9ff13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a5f79a9d094496a4313fc7f268300e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Embedding model\n",
    "# https://www.sbert.net/\n",
    "# pip3 install sentence-transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Text\n",
    "\n",
    "texts = [\n",
    "    \"Bali's beautiful beaches and rich culture stands out as a fantastic travel destination.\",\n",
    "    \"Pizza in Rome is famous for its thin crust, fresh ingredients and wood-fired ovens.\",\n",
    "    \"Graphics processing units (GPU) have become an essential foundation for artificial intelligence.\",\n",
    "    \"Newton's laws of motion transformed our understanding of physics.\",\n",
    "    \"The French Revolution played a crucial role in shaping contemporary France.\",\n",
    "    \"Maintaining good health requires regular exercise, balanced diet and quality sleep.\",\n",
    "    \"Dali's surrealistic artworks, like 'The Persistence of Memory,' captivate audiences with their dreamlike imagery and imaginative brilliance\",\n",
    "    \"Global warming threatens the planet's ecosystems and wildlife.\",\n",
    "    \"The KI-Servicezentrum Berlin-Brandenburg offers services such as consulting, workshops, MOOCs and computer resources.\",\n",
    "    \"Django Reinhardt's jazz compositions are celebrated for their captivating melodies and innovative guitar work..\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings type: <class 'numpy.ndarray'>\n",
      "Embeddings Matrix shape: (10, 384)\n"
     ]
    }
   ],
   "source": [
    "# Encode texts\n",
    "\n",
    "text_embeddings = model.encode(texts)\n",
    "\n",
    "print(\"Embeddings type:\", type(text_embeddings))\n",
    "print(\"Embeddings Matrix shape:\", text_embeddings.shape)\n",
    "\n",
    "# Note: \"all-MiniLM-L6-v2\" encodes texts up to 256 words. It’ll truncate any text longer than this.\n",
    "# https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### &bull; **Check Similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing Embeddings in a simple dict\n",
    "\n",
    "text_embs_dict = dict(zip(texts, list(text_embeddings)))\n",
    "# key: text, value: numpy array with embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define similarity metric\n",
    "\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(x, y):\n",
    "    return dot(x, y) / (norm(x) * norm(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cosine similarities for: 'I really need some vacations'\n",
      "\n",
      "Bali's beautiful beaches and rich culture stands out as a fantastic travel destination. 0.302\n",
      "Pizza in Rome is famous for its thin crust, fresh ingredients and wood-fired ovens. 0.161\n",
      "Graphics processing units (GPU) have become an essential foundation for artificial intelligence. -0.089\n",
      "Newton's laws of motion transformed our understanding of physics. 0.022\n",
      "The French Revolution played a crucial role in shaping contemporary France. 0.012\n",
      "Maintaining good health requires regular exercise, balanced diet and quality sleep. 0.176\n",
      "Dali's surrealistic artworks, like 'The Persistence of Memory,' captivate audiences with their dreamlike imagery and imaginative brilliance 0.022\n",
      "Global warming threatens the planet's ecosystems and wildlife. 0.105\n",
      "The KI-Servicezentrum Berlin-Brandenburg offers services such as consulting, workshops, MOOCs and computer resources. 0.105\n",
      "Django Reinhardt's jazz compositions are celebrated for their captivating melodies and innovative guitar work.. 0.066\n"
     ]
    }
   ],
   "source": [
    "# Check similarities\n",
    "\n",
    "test_text = \"I really need some vacations\"\n",
    "emb_test_text = model.encode(test_text)\n",
    "\n",
    "print(f\"\\nCosine similarities for: '{test_text}'\\n\")\n",
    "for k, v in text_embs_dict.items():\n",
    "    print(k, round(cos_sim(emb_test_text, v), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #dd6108; color: #ffffff; padding: 10px;\">\n",
    "<h3>2. Basics - A simple RAG tool</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "client = chromadb.Client()\n",
    "# client = chromadb.PersistentClient(path=\"chroma_data/\")\n",
    "# For a ChromaDB instance on the disk\n",
    "\n",
    "embedding_func = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=EMBEDDING_MODEL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "UniqueConstraintError",
     "evalue": "Collection demo_docs already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUniqueConstraintError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Init collection\u001b[39;00m\n\u001b[1;32m      3\u001b[0m COLLECTION_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdemo_docs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m collection \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCOLLECTION_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhnsw:space\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcosine\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/razer/Documents/Workshops/ragsst/.myvenv/lib/python3.10/site-packages/chromadb/api/client.py:117\u001b[0m, in \u001b[0;36mClient.create_collection\u001b[0;34m(self, name, configuration, metadata, embedding_function, data_loader, get_or_create)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_collection\u001b[39m(\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m     get_or_create: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    116\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Collection:\n\u001b[0;32m--> 117\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_server\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtenant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mget_or_create\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_or_create\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfiguration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Collection(\n\u001b[1;32m    126\u001b[0m         client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server,\n\u001b[1;32m    127\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    128\u001b[0m         embedding_function\u001b[38;5;241m=\u001b[39membedding_function,\n\u001b[1;32m    129\u001b[0m         data_loader\u001b[38;5;241m=\u001b[39mdata_loader,\n\u001b[1;32m    130\u001b[0m     )\n",
      "File \u001b[0;32m/mnt/c/Users/razer/Documents/Workshops/ragsst/.myvenv/lib/python3.10/site-packages/chromadb/telemetry/opentelemetry/__init__.py:146\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/mnt/c/Users/razer/Documents/Workshops/ragsst/.myvenv/lib/python3.10/site-packages/chromadb/api/segment.py:176\u001b[0m, in \u001b[0;36mSegmentAPI.create_collection\u001b[0;34m(self, name, configuration, metadata, get_or_create, tenant, database)\u001b[0m\n\u001b[1;32m    164\u001b[0m model \u001b[38;5;241m=\u001b[39m CollectionModel(\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m,\n\u001b[1;32m    166\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m     dimension\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    174\u001b[0m )\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# TODO: Let sysdb create the collection directly from the model\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m coll, created \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sysdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_configuration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdimension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# This is lazily populated on the first add\u001b[39;49;00m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mget_or_create\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_or_create\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# TODO: wrap sysdb call in try except and log error if it fails\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created:\n",
      "File \u001b[0;32m/mnt/c/Users/razer/Documents/Workshops/ragsst/.myvenv/lib/python3.10/site-packages/chromadb/telemetry/opentelemetry/__init__.py:146\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/mnt/c/Users/razer/Documents/Workshops/ragsst/.myvenv/lib/python3.10/site-packages/chromadb/db/mixins/sysdb.py:229\u001b[0m, in \u001b[0;36mSqlSysDB.create_collection\u001b[0;34m(self, id, name, configuration, metadata, dimension, get_or_create, tenant, database)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    223\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_collections(\n\u001b[1;32m    224\u001b[0m                 \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39mcollection[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m], tenant\u001b[38;5;241m=\u001b[39mtenant, database\u001b[38;5;241m=\u001b[39mdatabase\n\u001b[1;32m    225\u001b[0m             )[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    226\u001b[0m             \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    227\u001b[0m         )\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 229\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UniqueConstraintError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollection \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already exists\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    231\u001b[0m collection \u001b[38;5;241m=\u001b[39m Collection(\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m,\n\u001b[1;32m    233\u001b[0m     name\u001b[38;5;241m=\u001b[39mname,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    239\u001b[0m     version\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    240\u001b[0m )\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtx() \u001b[38;5;28;01mas\u001b[39;00m cur:\n",
      "\u001b[0;31mUniqueConstraintError\u001b[0m: Collection demo_docs already exists"
     ]
    }
   ],
   "source": [
    "# Init collection\n",
    "\n",
    "COLLECTION_NAME = \"demo_docs\"\n",
    "\n",
    "collection = client.create_collection(\n",
    "    name=COLLECTION_NAME,\n",
    "    embedding_function=embedding_func,\n",
    "    metadata={\"hnsw:space\": \"cosine\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event CollectionAddEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    }
   ],
   "source": [
    "# Create collection\n",
    "# (Adding metadata. Optional)\n",
    "\n",
    "topics = [\n",
    "    \"travel\",\n",
    "    \"food\",\n",
    "    \"technology\",\n",
    "    \"science\",\n",
    "    \"history\",\n",
    "    \"health\",\n",
    "    \"painting\",\n",
    "    \"climate change\",\n",
    "    \"business\",\n",
    "    \"music\",\n",
    "]\n",
    "\n",
    "collection.add(\n",
    "    documents=texts,\n",
    "    ids=[f\"id{i}\" for i in range(len(texts))],\n",
    "    metadatas=[{\"topic\": topic} for topic in topics],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query dict keys:\n",
      "dict_keys(['ids', 'distances', 'metadatas', 'embeddings', 'documents', 'uris', 'data', 'included'])\n",
      "\n",
      "Query: I am looking for something to eat\n",
      "\n",
      "Results:\n",
      "id: id1\n",
      "Text: Pizza in Rome is famous for its thin crust, fresh ingredients and wood-fired ovens.\n",
      "Distance: 0.72\n",
      "Metadata: {'topic': 'food'}\n",
      "id: id5\n",
      "Text: Maintaining good health requires regular exercise, balanced diet and quality sleep.\n",
      "Distance: 0.85\n",
      "Metadata: {'topic': 'health'}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: What's going on in the world?\n",
      "\n",
      "Results:\n",
      "id: id7\n",
      "Text: Global warming threatens the planet's ecosystems and wildlife.\n",
      "Distance: 0.74\n",
      "Metadata: {'topic': 'climate change'}\n",
      "id: id3\n",
      "Text: Newton's laws of motion transformed our understanding of physics.\n",
      "Distance: 0.86\n",
      "Metadata: {'topic': 'science'}\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Query: Great jazz player\n",
      "\n",
      "Results:\n",
      "id: id9\n",
      "Text: Django Reinhardt's jazz compositions are celebrated for their captivating melodies and innovative guitar work..\n",
      "Distance: 0.52\n",
      "Metadata: {'topic': 'music'}\n",
      "id: id4\n",
      "Text: The French Revolution played a crucial role in shaping contemporary France.\n",
      "Distance: 0.88\n",
      "Metadata: {'topic': 'history'}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Quering the Database\n",
    "\n",
    "query1 = \"I am looking for something to eat\"\n",
    "query2 = \"What's going on in the world?\"\n",
    "query3 = \"Great jazz player\"\n",
    "\n",
    "queries = [query1, query2, query3]\n",
    "\n",
    "query_results = collection.query(\n",
    "    query_texts=queries,  # list of strings or just one element (string)\n",
    "    n_results=2,\n",
    ")\n",
    "print(\"Query dict keys:\")\n",
    "print(query_results.keys())\n",
    "\n",
    "for i in range(len(queries)):\n",
    "    print(\"\\nQuery:\", queries[i])\n",
    "\n",
    "    print(\"\\nResults:\")\n",
    "    for j in range(len(query_results[\"ids\"][i])):\n",
    "        print(\"id:\", query_results[\"ids\"][i][j])\n",
    "        print(\"Text:\", query_results[\"documents\"][i][j])\n",
    "        print(\"Distance:\", round(query_results[\"distances\"][i][j], 2))\n",
    "        print(\"Metadata:\", query_results[\"metadatas\"][i][j])\n",
    "\n",
    "    print(80 * \"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### &bull; **Running a Large Language Model locally with Ollama**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "from os import getenv\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# $ ollama serve\n",
    "BASEURL = urljoin(getenv(\"OLLAMA_HOST\", \"http://localhost:11434\"), \"api\")\n",
    "MODEL = \"llama3.2\"\n",
    "\n",
    "\n",
    "def generate(prompt, context=[], top_k=5, top_p=0.9, temp=0.5):\n",
    "    url = BASEURL + \"/generate\"\n",
    "    data = {\n",
    "        \"prompt\": prompt,\n",
    "        \"model\": MODEL,\n",
    "        \"stream\": False,\n",
    "        \"context\": context,\n",
    "        \"options\": {\"temperature\": temp, \"top_p\": top_p, \"top_k\": top_k},\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        r = requests.post(url, json=data)\n",
    "        response_dic = json.loads(r.text)\n",
    "        return response_dic.get('response', ''), response_dic.get('context', '')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are many things happening in the world, and I'll try to give you a brief overview. Keep in mind that my knowledge cutoff is December 2023, so I might not have information on very recent events.\n",
      "\n",
      "**Global News:**\n",
      "\n",
      "1. **Climate Change:** The Intergovernmental Panel on Climate Change (IPCC) has warned that the world has only about a decade to take drastic action to limit global warming to 1.5°C above pre-industrial levels.\n",
      "2. **Conflict and Refugees:** Tensions remain high in Ukraine, with ongoing fighting between Ukrainian forces and Russian-backed separatists. The refugee crisis continues to affect many countries, including Syria, Yemen, and Afghanistan.\n",
      "3. **Economy and Trade:** Global trade tensions have eased somewhat, but the impact of the COVID-19 pandemic is still being felt. The US-China trade war has been a significant factor in this.\n",
      "\n",
      "**Science and Technology:**\n",
      "\n",
      "1. **Space Exploration:** NASA's Artemis program aims to return humans to the Moon by 2025 and establish a sustainable presence on the lunar surface.\n",
      "2. **Artificial Intelligence:** AI research is advancing rapidly, with applications in areas like healthcare, finance, and transportation.\n",
      "3. **Renewable Energy:** Solar and wind energy are becoming increasingly cost-competitive with fossil fuels, driving growth in renewable energy adoption.\n",
      "\n",
      "**Health:**\n",
      "\n",
      "1. **COVID-19 Pandemic:** The pandemic continues to evolve, with new variants emerging and vaccination efforts ongoing worldwide.\n",
      "2. **Global Health Security:** Concerns remain about the spread of infectious diseases, such as malaria and tuberculosis, particularly in low-income countries.\n",
      "3. **Mental Health:** Mental health has become a growing concern globally, with increased awareness and support for mental wellness.\n",
      "\n",
      "**Other Notable Events:**\n",
      "\n",
      "1. **European Elections:** The European Parliament elections took place in May 2024, with the results shaping the future of European politics.\n",
      "2. **US Midterm Elections:** The US midterm elections will take place in November 2024, influencing the balance of power in Congress and state governments.\n",
      "3. **G20 Summit:** The G20 summit will be held in 2024, bringing together world leaders to address pressing global issues.\n",
      "\n",
      "These are just a few examples of what's happening in the world right now. If you have specific questions or topics you'd like me to expand on, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "llm_response, _ = generate(\"What's going on in the world?\",  \n",
    "                           top_k=10, top_p=0.9, temp=0.5)\n",
    "print(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/razer/Documents/Workshops/ragsst/.myvenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What's going on in the world?\n",
      "\n",
      "Relevant Sources:\n",
      "1. Global warming threatens the planet's ecosystems and wildlife.\n",
      "2. Newton's laws of motion transformed our understanding of physics.\n",
      "\n",
      "RAG Response:\n",
      "It appears that there are two different topics being discussed here. \n",
      "\n",
      "The first context mentions global warming, which is a pressing environmental issue affecting the planet's ecosystems and wildlife. This suggests that something concerning or alarming is happening in the world related to climate change.\n",
      "\n",
      "The second context refers to Newton's laws of motion, which revolutionized our understanding of physics. This implies that there are scientific developments or advancements happening in the world related to physics and science.\n",
      "\n",
      "Given these two contexts, it seems like the answer would be that both global warming (Context 1) and scientific progress (specifically, advances in physics, as hinted at by Newton's laws of motion, Context 2) are happening in the world.\n"
     ]
    }
   ],
   "source": [
    "def rag_generate(query, n_results=3, top_k=10, top_p=0.9, temp=0.5):\n",
    "    \"\"\"\n",
    "    RAG function: Retrieve relevant context and generate response\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve relevant documents\n",
    "    search_results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=n_results\n",
    "    )\n",
    "    \n",
    "    # Step 2: Extract the relevant text passages\n",
    "    relevant_docs = search_results['documents'][0]\n",
    "    \n",
    "    # Step 3: Create context from retrieved documents\n",
    "    context_text = \"\\n\\n\".join([f\"Context {i+1}: {doc}\" for i, doc in enumerate(relevant_docs)])\n",
    "    \n",
    "    # Step 4: Create enhanced prompt with context\n",
    "    enhanced_prompt = f\"\"\"Based on the following context information, please answer the question.\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Step 5: Generate response using the enhanced prompt\n",
    "    response, ollama_context = generate(enhanced_prompt, top_k=top_k, top_p=top_p, temp=temp)\n",
    "    \n",
    "    return response, relevant_docs\n",
    "\n",
    "# Test the RAG system\n",
    "user_query = \"What's going on in the world?\"\n",
    "rag_response, sources = rag_generate(user_query, n_results=2)\n",
    "\n",
    "print(f\"Query: {user_query}\")\n",
    "print(f\"\\nRelevant Sources:\")\n",
    "for i, source in enumerate(sources, 1):\n",
    "    print(f\"{i}. {source}\")\n",
    "\n",
    "print(f\"\\nRAG Response:\")\n",
    "print(rag_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " -----------------------------------------------------------------------------\n",
    "\n",
    " ## Integration\n",
    "\n",
    " #### **Integrate the components for a RAG System.**\n",
    "\n",
    " ### References:\n",
    "\n",
    " #### &bull; **Text embedding: Sentence Bert**\n",
    "\n",
    " https://www.sbert.net/\n",
    "\n",
    "\n",
    "\n",
    " Sample Sentence Transformers:\n",
    "\n",
    "\n",
    "\n",
    " https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "\n",
    "\n",
    "\n",
    " https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1\n",
    "\n",
    "\n",
    "\n",
    " https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-cos-v1\n",
    "\n",
    "\n",
    "\n",
    " Check and compare\n",
    "\n",
    " #### &bull; **Vector Database: ChromaDB**\n",
    "\n",
    " https://docs.trychroma.com/\n",
    "\n",
    " #### &bull; **Local LLM: ollama**\n",
    "\n",
    " https://ollama.ai/\n",
    "\n",
    " #### &bull; **Frontend: Gradio**\n",
    "\n",
    " https://www.gradio.app/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ragsst .myvenv)",
   "language": "python",
   "name": "ragsst-myvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
